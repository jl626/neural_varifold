{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Library dependencies (include jax, neural_tangents)\n",
    "# note that there are two numpy lib (onp - standard numpy library // np - Jax competible autodiff implmentation of numpy)\n",
    "import sys\n",
    "sys.path.append( '..' )\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "import neural_tangents as nt\n",
    "from neural_tangents import stax\n",
    "from neural_tangents.utils.kernel import Kernel\n",
    "import numpy as onp\n",
    "from jax import random\n",
    "from jax.config import config ; config.update('jax_enable_x64', True)\n",
    "import util\n",
    "from util import Prod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# GPU device check\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varifold Classification example\n",
    "\n",
    "Here, we compute ``neural varifold`` using Charon-Trouve style kernel on NTK (neural tangent kernel) \n",
    "\n",
    "$\\Theta_{varifold} = \\Theta_{position} \\otimes \\Theta_{normal}$\n",
    "\n",
    "This is called ``neural'' because NTK is equivalent to training infinite-width neural network with MSE (L2) loss. \n",
    "We are going to demonstrate its capability of classification on ModelNet10 dataset. First of all, we are going to load ModelNet10 dataset (3991 train // 908 test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data parameter setting \n",
    "n_pts = 1024 # number of points \n",
    "onp.random.seed(0) # random seed for numpy\n",
    "sample_size = 1\n",
    "\n",
    "# Load the dataset\n",
    "data = onp.load('./modelnet/modelnet10_1024_core3.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on ModelNet10 is computationally costly as the computational costs of the kernel is quatratic with respect to the size of data (3991//908). \n",
    "\n",
    "* In this demo, we are going to reduce the size of training data by random subsampling 10 examples per each class (i.e. training data size = 100)\n",
    "* We are going to retain all test data (test data size = 908). \n",
    "* note that test data size shown below is 920. It is because input data size must be decimal for GPU parallel batch processing. So we duplicate the orignial test data (908) to fit the size requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first - train dataset\n",
    "x_train = data['x_train'][:3991]\n",
    "y_train = data['y_train'][:3991]    \n",
    "y_label = np.argmax(y_train,axis=-1)\n",
    "\n",
    "# subsample training data  \n",
    "new_train = []\n",
    "new_label = []\n",
    "new_graph = []\n",
    "for i in range(10):\n",
    "    idx = onp.argwhere(y_label==i)[:,0]\n",
    "    sample = onp.random.choice(len(idx),sample_size,replace=False)\n",
    "    new_train.append(x_train[idx[sample]])\n",
    "    new_label.append(y_train[idx[sample]])\n",
    "    #new_graph.append( graph1[idx[sample]])\n",
    "\n",
    "# train data\n",
    "x_train = onp.concatenate(new_train,0)\n",
    "y_train = onp.concatenate(new_label,0)\n",
    "\n",
    "# test (note that it is actually 908 samples + duplication to fit hardward requirement)\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test'] \n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that train/test data are tensor of $sample size \\times point size \\times 6$, \n",
    "where, sample size is the size of train/test samples, and point size is the number of points representing individual shapes.\n",
    "\n",
    "\n",
    "In this experiment, each sample has 1024 points with 6 features. \n",
    "\n",
    "* First three features are $x,y,z$ positional coordinates \n",
    "* Last three features are corresponding unit normal coordinates $nx,ny,nz$. \n",
    "\n",
    "\n",
    "In order to feed the above data into the neural varifold kernel, we are going to reshape the tensor as $ sample size \\times point size \\times 2 \\times 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(sample_size*10,n_pts,2,3).transpose((0,2,1,3))\n",
    "x_test  = x_test.reshape(920,n_pts,2,3).transpose((0,2,1,3))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show an example of the train/test data. We are going to plot a sample from each class in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample from each class in the training data \n",
    "for i in range(10):\n",
    "    sample = x_train[i*sample_size+0,0].squeeze()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(sample[:,0],sample[:,1],sample[:,2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build an infinite-width neural network using ``neural_tangent`` library. \n",
    "\n",
    "In this example, we are going to use PointNet like architecture with LayerNorm and Global Average Pooling (GAP). \n",
    "\n",
    "``stax.Dense`` function has three key parameters ``p1`` = number of channels, ``p2`` = weight initialisation parameter, ``p3`` = bias initialisation parameter.\n",
    "\n",
    "* ``p1``: since we are using infinite-width, we just set the number of channels does not matter. In this example, we set it as 1 \n",
    "* ``p2``: In many NTK literature, it is usually bounded between 1 ~ 2. Here, we set it as 1 for the simplicity. \n",
    "* ``p3``: In many NTK liternature, its value has usually a value around 0.0 ~ 0.1. Here, we set it as 100. Surprisingly it works much better for NTK. \n",
    "\n",
    "\n",
    "``stax.Conv`` function has 6 key parameters in our case. ``p1`` = number of channels, ``p2`` = 2D convolution window, ``p3`` = padding, ``p4`` = convolution method (either valid or same), ``p5`` = weight initialisation parameter, ``p6`` = bias initialisation parameter \n",
    "\n",
    "* ``p1``: Just like ``stax.Dense`` infinite-width cases, channel size does not really matter. We set it as 1. \n",
    "* ``p2``: The convolution window size is (1,1) Given Our data input dimension (num_batch_size X num_point_size X 2 X 3), 1 X 1 convolution is doing 1D convolution on position and its normal separately. \n",
    "* ``p3``: We don't use padding. i.e. (1,1). \n",
    "* ``p4``: Since we are using (1,1) convolution either valid or same methods return the same output. \n",
    "* ``p5`` and ``p6``: we are using the same parameters as the ``stax.Dense``.  \n",
    "\n",
    "``init_fn``, ``apply_fn`` and ``kernel_fn`` are correponding to initialisation, finite-width NN, NTK representation of the infinite-width neural network (In this experiment)\n",
    "\n",
    "``Prod`` does the element-wise product between position and normal kernel  $\\Theta_{position} \\otimes \\Theta_{normal}$\n",
    "\n",
    "``stax.GlobalAvgPool`` compute a varifold norm by averaging the elements of the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the 5-layer infinite network.\n",
    "layers = []\n",
    "for k in range(5):\n",
    "    layers += [stax.Dense(1, 1., 0.05),stax.LayerNorm(), stax.Relu()] # PointNet like architecture with fully connected layer\n",
    "    #layers += [stax.Conv(1, (1,1), (1,1), 'SAME', 1., 0.05),stax.LayerNorm(), stax.Relu()] # PointNet like architecture with pointwise convolution\n",
    "print(len(layers)//3, '-layer network')\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(*(layers + [Prod(),stax.GlobalAvgPool()])) # element-wise product to compute varifold kernel, then compute average to get a norm \n",
    "\n",
    "# compute the kernel in batches, in parallel.\n",
    "kernel_fn = nt.batch(kernel_fn,device_count=-1,batch_size=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like traditional kernel methods, NTK forms Gram Matrices between training data $K_{(train,train)}$ and training & testing data $K_{(test,train)}$\n",
    "\n",
    "Given two gram matrices, shape classification can be reformulated as a kernel regression problem. \n",
    "\n",
    "``gradient_descent_mse_ensemble`` function computes the kernel regression with a diagional regularisation parameter $\\lambda$. In this experiment, we set it as 1e-2\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{test} = K_{(test,train)}  (K_{(train,train)} + \\lambda I )^{-1}  Y_{train}\n",
    "\\end{equation}\n",
    "\n",
    "where, $Y_{train}$ and $Y_{test}$ are ground truth label for train and test data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct kernel regression with diagonal regularisation (1e-2)\n",
    "predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, x_train,y_train, diag_reg=1e-2) # train a function\n",
    "fx_test_nngp, fx_test_ntk = predict_fn(x_test=x_test) # regression on the test dataset\n",
    "fx_test_nngp.block_until_ready()\n",
    "fx_test_ntk.block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NTK technique is equivalent with training corresponding infinite-width neural network with MSE (L2) loss. \n",
    "Here, we are going to compute MSE loss and its classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out accuracy and loss for infinite network predictions.\n",
    "loss = lambda fx, y_hat: 0.5 * np.mean((fx - y_hat) ** 2)\n",
    "util.print_summary('NNGP test', y_test[:908], fx_test_nngp[:908], None, loss)\n",
    "util.print_summary('NTK  test', y_test[:908], fx_test_ntk[:908] , None, loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d12ef22fb8f524c618e5af0efbf3ae226e90001e61efdab00d0d0eab3a6f547"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
