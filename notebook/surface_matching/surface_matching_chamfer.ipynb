{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '../..' )\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "import numpy as np\n",
    "from energy import tangent_kernel\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be slow!\")\n",
    "import input_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_name = 'cup'\n",
    "#experiment_name = 'dolphin'\n",
    "experiment_name = 'hippo'\n",
    "#experiment_name = 'bunny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the target 3D model using load_obj\n",
    "# case 1.  \"Cup\"\n",
    "low = 31\n",
    "mid = 26\n",
    "high = 20\n",
    "if experiment_name=='cup':\n",
    "    # two different Cups \n",
    "    [VS,FS,FunS] = input_output.loadData(\"../../data/matching/cup2.ply\")\n",
    "    [VT,FT,FunT] = input_output.loadData(\"../../data/matching/cup3_broken.ply\")\n",
    "    source = [VS,FS]\n",
    "    target= [VT,FT]\n",
    "    # original size\n",
    "    print(VS.shape)\n",
    "    print(VT.shape)\n",
    "    # Option 1 Sampling\n",
    "    # Decimate source mesh to compute initialization for the multires algorithm \n",
    "    param_decimation = {'factor':31/32,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 32\n",
    "    [verts1,faces1]= input_output.decimate_mesh(VS,FS,param_decimation)\n",
    "    print(verts1.shape)\n",
    "    param_decimation = {'factor':31/32,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 32\n",
    "    [verts2,faces2]= input_output.decimate_mesh(VT,FT,param_decimation)\n",
    "    print(verts2.shape)\n",
    "    verts1 = torch.FloatTensor(verts1)\n",
    "    verts2 = torch.FloatTensor(verts2)\n",
    "    faces1 = torch.LongTensor(faces1)\n",
    "    faces2 = torch.LongTensor(faces2)\n",
    "    save_obj('../../results/cup2_mid.obj', verts1, faces1)\n",
    "    save_obj('../../results/cup3_broken_mid.obj', verts2, faces2)\n",
    "# Case 2 Dolphin\n",
    "elif experiment_name == 'dolphin':\n",
    "    # sphere to dolphine \n",
    "    src_mesh = ico_sphere(4)\n",
    "    VT, FT, FunS = load_obj(\"../../data/matching/dolphin.obj\")\n",
    "    VS, FS = src_mesh.verts_packed(), src_mesh.faces_packed()\n",
    "    verts1, faces1 = VS, FS\n",
    "    verts2, faces2 = VT, FT.verts_idx\n",
    "    \n",
    "# Case 3 Hippocampus \n",
    "elif experiment_name == 'hippo':\n",
    "    verts1, faces1, verts2, faces2 = torch.load('../../data/matching/hippos_red.pt')\n",
    "\n",
    "# Case 2 Dolphin\n",
    "elif experiment_name == 'bunny':\n",
    "    # sphere to dolphine \n",
    "    src_mesh = ico_sphere(4)\n",
    "    VT, FT, FunS = load_obj(\"../../data/matching/bunny.obj\")\n",
    "    VS, FS = src_mesh.verts_packed(), src_mesh.faces_packed()\n",
    "    # Decimate source mesh to compute initialization for the multires algorithm \n",
    "    param_decimation = {'factor':17/32,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 16\n",
    "    [verts1,faces1]= input_output.decimate_mesh(VS.numpy(),FS.numpy(),param_decimation)\n",
    "    print(verts1.shape)\n",
    "    param_decimation = {'factor':31/32,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 16\n",
    "    [verts2,faces2]= input_output.decimate_mesh(VT.numpy(),FT.verts_idx.numpy(),param_decimation)\n",
    "    print(verts2.shape)\n",
    "    verts1 = torch.FloatTensor(verts1)\n",
    "    verts2 = torch.FloatTensor(verts2)\n",
    "    faces1 = torch.LongTensor(faces1)\n",
    "    faces2 = torch.LongTensor(faces2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1654, 3])\n",
      "torch.Size([1654, 3])\n"
     ]
    }
   ],
   "source": [
    "# verts is a FloatTensor of shape (V, 3) where V is the number of vertices in the mesh\n",
    "# faces is an object which contains the following LongTensors: verts_idx, normals_idx and textures_idx\n",
    "# For this tutorial, normals and textures are ignored.\n",
    "faces_idx1 = faces1.to(device)\n",
    "faces_idx2 = faces2.to(device)\n",
    "\n",
    "# Mark: obj files\n",
    "#faces_idx1 = faces1.to(device)#.verts_idx.to(device)\n",
    "#faces_idx2 = faces2.verts_idx.to(device)\n",
    "\n",
    "verts1 = verts1.to(device)\n",
    "verts2 = verts2.to(device)\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 centered at (0,0,0). \n",
    "# (scale, center) will be used to bring the predicted mesh to its original center and scale\n",
    "# Note that normalizing the target mesh, speeds up the optimization but is not necessary!\n",
    "#'''\n",
    "center1 = verts1.mean(0)\n",
    "center2 = verts2.mean(0)\n",
    "verts1 = verts1 - center1\n",
    "verts2 = verts2 - center2\n",
    "scale1 = max(verts1.abs().max(0)[0])\n",
    "scale2 = max(verts2.abs().max(0)[0])\n",
    "verts1 = verts1 / scale1\n",
    "verts2 = verts2 / scale2\n",
    "#'''\n",
    "# We construct a Meshes structure for the target mesh\n",
    "src_mesh = Meshes(verts=[verts1], faces=[faces_idx1])\n",
    "trg_mesh = Meshes(verts=[verts2], faces=[faces_idx2])\n",
    "print(verts1.shape)\n",
    "print(verts2.shape)\n",
    "#save_obj('../../results/cup2.obj',verts1,faces1)\n",
    "#save_obj('../../results/cup3_broken.obj',verts2,faces2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testnet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(6,64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64,128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128,3))\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setting\n",
    "models = testnet().cuda()\n",
    "optimizer = torch.optim.Adam(models.parameters(), lr=.001)\n",
    "# Number of optimization steps\n",
    "Niter = 200001\n",
    "\n",
    "# loss parameters\n",
    "# Weight for the chamfer loss\n",
    "w_chamfer = 1.0 \n",
    "# Plot period for the losses\n",
    "plot_period = 1000\n",
    "\n",
    "chamfer_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_engine(V1,V2,L1,L2,K):\n",
    "    cst_tmp = []\n",
    "    n_batch = 10000\n",
    "    for i in range(len(V1)//n_batch + 1):\n",
    "        tmp = V1[i*n_batch:(i+1)*n_batch,:]\n",
    "        l_tmp = L1[i*n_batch:(i+1)*n_batch,:]\n",
    "        v = torch.matmul(K(tmp,V2),L2)*l_tmp\n",
    "        cst_tmp.append(v)\n",
    "    cst = torch.sum(torch.cat(cst_tmp,0))\n",
    "    return cst\n",
    "\n",
    "def CompCLNn(F, V):\n",
    "    if F.shape[1] == 2:\n",
    "        V0, V1 = V.index_select(0, F[:, 0]), V.index_select(0, F[:, 1])\n",
    "        C, N  =  (V0 + V1)/2, V1 - V0\n",
    "    else:\n",
    "        V0, V1, V2 = V.index_select(0, F[:, 0]), V.index_select(0, F[:, 1]), V.index_select(0, F[:, 2])\n",
    "        C, N =  (V0 + V1 + V2)/3, .5 * torch.cross(V1 - V0, V2 - V0)\n",
    "\n",
    "    L = (N ** 2).sum(dim=1)[:, None].sqrt()\n",
    "    return C, L, N / L, 1#Fun_F\n",
    "\n",
    "c,l,n,_ = CompCLNn(faces_idx1,verts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Iter: total_loss 0.153925 Chamfer_loss 0.153925\n",
      "current best loss is 0: 0.153925\n",
      "1000 Iter: total_loss 0.025799 Chamfer_loss 0.025799\n",
      "current best loss is 997: 0.025590\n",
      "2000 Iter: total_loss 0.023101 Chamfer_loss 0.023101\n",
      "current best loss is 1995: 0.022932\n",
      "3000 Iter: total_loss 0.021752 Chamfer_loss 0.021752\n",
      "current best loss is 2988: 0.021454\n",
      "4000 Iter: total_loss 0.021222 Chamfer_loss 0.021222\n",
      "current best loss is 3891: 0.020798\n",
      "5000 Iter: total_loss 0.020432 Chamfer_loss 0.020432\n",
      "current best loss is 4939: 0.020292\n",
      "6000 Iter: total_loss 0.019983 Chamfer_loss 0.019983\n",
      "current best loss is 5994: 0.019860\n",
      "7000 Iter: total_loss 0.019922 Chamfer_loss 0.019922\n",
      "current best loss is 6838: 0.019542\n",
      "8000 Iter: total_loss 0.019444 Chamfer_loss 0.019444\n",
      "current best loss is 7701: 0.019353\n",
      "9000 Iter: total_loss 0.019252 Chamfer_loss 0.019252\n",
      "current best loss is 8956: 0.019029\n",
      "10000 Iter: total_loss 0.018857 Chamfer_loss 0.018857\n",
      "current best loss is 9858: 0.018843\n",
      "11000 Iter: total_loss 0.018734 Chamfer_loss 0.018734\n",
      "current best loss is 10837: 0.018598\n",
      "12000 Iter: total_loss 0.018702 Chamfer_loss 0.018702\n",
      "current best loss is 11847: 0.018473\n",
      "13000 Iter: total_loss 0.018734 Chamfer_loss 0.018734\n",
      "current best loss is 12950: 0.018334\n",
      "14000 Iter: total_loss 0.019238 Chamfer_loss 0.019238\n",
      "current best loss is 13459: 0.018218\n",
      "15000 Iter: total_loss 0.018337 Chamfer_loss 0.018337\n",
      "current best loss is 14805: 0.018076\n",
      "16000 Iter: total_loss 0.018205 Chamfer_loss 0.018205\n",
      "current best loss is 15529: 0.017931\n",
      "17000 Iter: total_loss 0.018308 Chamfer_loss 0.018308\n",
      "current best loss is 16963: 0.017884\n",
      "18000 Iter: total_loss 0.018346 Chamfer_loss 0.018346\n",
      "current best loss is 17911: 0.017751\n",
      "19000 Iter: total_loss 0.017964 Chamfer_loss 0.017964\n",
      "current best loss is 18967: 0.017749\n",
      "20000 Iter: total_loss 0.018101 Chamfer_loss 0.018101\n",
      "current best loss is 19972: 0.017660\n",
      "21000 Iter: total_loss 0.018081 Chamfer_loss 0.018081\n",
      "current best loss is 20370: 0.017636\n",
      "22000 Iter: total_loss 0.017993 Chamfer_loss 0.017993\n",
      "current best loss is 21793: 0.017590\n",
      "23000 Iter: total_loss 0.018225 Chamfer_loss 0.018225\n",
      "current best loss is 22739: 0.017480\n",
      "24000 Iter: total_loss 0.017646 Chamfer_loss 0.017646\n",
      "current best loss is 23707: 0.017444\n",
      "25000 Iter: total_loss 0.017638 Chamfer_loss 0.017638\n",
      "current best loss is 24705: 0.017375\n",
      "26000 Iter: total_loss 0.018144 Chamfer_loss 0.018144\n",
      "current best loss is 25198: 0.017343\n",
      "27000 Iter: total_loss 0.017579 Chamfer_loss 0.017579\n",
      "current best loss is 26995: 0.017291\n",
      "28000 Iter: total_loss 0.017487 Chamfer_loss 0.017487\n",
      "current best loss is 27621: 0.017271\n",
      "29000 Iter: total_loss 0.017302 Chamfer_loss 0.017302\n",
      "current best loss is 28933: 0.017249\n",
      "30000 Iter: total_loss 0.017722 Chamfer_loss 0.017722\n",
      "current best loss is 29779: 0.017184\n",
      "31000 Iter: total_loss 0.017957 Chamfer_loss 0.017957\n",
      "current best loss is 30706: 0.017163\n",
      "32000 Iter: total_loss 0.017388 Chamfer_loss 0.017388\n",
      "current best loss is 31708: 0.017102\n",
      "33000 Iter: total_loss 0.017367 Chamfer_loss 0.017367\n",
      "current best loss is 32175: 0.017001\n",
      "34000 Iter: total_loss 0.017584 Chamfer_loss 0.017584\n",
      "current best loss is 33782: 0.016981\n",
      "35000 Iter: total_loss 0.017437 Chamfer_loss 0.017437\n",
      "current best loss is 34522: 0.016928\n",
      "36000 Iter: total_loss 0.016959 Chamfer_loss 0.016959\n",
      "current best loss is 35521: 0.016868\n",
      "37000 Iter: total_loss 0.017527 Chamfer_loss 0.017527\n",
      "current best loss is 36981: 0.016858\n",
      "38000 Iter: total_loss 0.017093 Chamfer_loss 0.017093\n",
      "current best loss is 37436: 0.016821\n",
      "39000 Iter: total_loss 0.017079 Chamfer_loss 0.017079\n",
      "current best loss is 38713: 0.016749\n",
      "40000 Iter: total_loss 0.017516 Chamfer_loss 0.017516\n",
      "current best loss is 38713: 0.016749\n",
      "41000 Iter: total_loss 0.016888 Chamfer_loss 0.016888\n",
      "current best loss is 40848: 0.016692\n",
      "42000 Iter: total_loss 0.017238 Chamfer_loss 0.017238\n",
      "current best loss is 41664: 0.016674\n",
      "43000 Iter: total_loss 0.016935 Chamfer_loss 0.016935\n",
      "current best loss is 42896: 0.016658\n",
      "44000 Iter: total_loss 0.016894 Chamfer_loss 0.016894\n",
      "current best loss is 43682: 0.016650\n",
      "45000 Iter: total_loss 0.017183 Chamfer_loss 0.017183\n",
      "current best loss is 44807: 0.016590\n",
      "46000 Iter: total_loss 0.017244 Chamfer_loss 0.017244\n",
      "current best loss is 45055: 0.016562\n",
      "47000 Iter: total_loss 0.016999 Chamfer_loss 0.016999\n",
      "current best loss is 46400: 0.016517\n",
      "48000 Iter: total_loss 0.016886 Chamfer_loss 0.016886\n",
      "current best loss is 47913: 0.016512\n",
      "49000 Iter: total_loss 0.016793 Chamfer_loss 0.016793\n",
      "current best loss is 48063: 0.016493\n",
      "50000 Iter: total_loss 0.016903 Chamfer_loss 0.016903\n",
      "current best loss is 49908: 0.016472\n",
      "51000 Iter: total_loss 0.016920 Chamfer_loss 0.016920\n",
      "current best loss is 50639: 0.016463\n",
      "52000 Iter: total_loss 0.016697 Chamfer_loss 0.016697\n",
      "current best loss is 51531: 0.016459\n",
      "53000 Iter: total_loss 0.017132 Chamfer_loss 0.017132\n",
      "current best loss is 52616: 0.016432\n",
      "54000 Iter: total_loss 0.016597 Chamfer_loss 0.016597\n",
      "current best loss is 53968: 0.016409\n",
      "55000 Iter: total_loss 0.016600 Chamfer_loss 0.016600\n",
      "current best loss is 54803: 0.016352\n",
      "56000 Iter: total_loss 0.016981 Chamfer_loss 0.016981\n",
      "current best loss is 54803: 0.016352\n",
      "57000 Iter: total_loss 0.016626 Chamfer_loss 0.016626\n",
      "current best loss is 56240: 0.016269\n",
      "58000 Iter: total_loss 0.016851 Chamfer_loss 0.016851\n",
      "current best loss is 56240: 0.016269\n",
      "59000 Iter: total_loss 0.016600 Chamfer_loss 0.016600\n",
      "current best loss is 56240: 0.016269\n",
      "60000 Iter: total_loss 0.016346 Chamfer_loss 0.016346\n",
      "current best loss is 59176: 0.016258\n",
      "61000 Iter: total_loss 0.016431 Chamfer_loss 0.016431\n",
      "current best loss is 60073: 0.016211\n",
      "62000 Iter: total_loss 0.016591 Chamfer_loss 0.016591\n",
      "current best loss is 60073: 0.016211\n",
      "63000 Iter: total_loss 0.016988 Chamfer_loss 0.016988\n",
      "current best loss is 62801: 0.016178\n",
      "64000 Iter: total_loss 0.016596 Chamfer_loss 0.016596\n",
      "current best loss is 62801: 0.016178\n",
      "65000 Iter: total_loss 0.016569 Chamfer_loss 0.016569\n",
      "current best loss is 62801: 0.016178\n",
      "66000 Iter: total_loss 0.016638 Chamfer_loss 0.016638\n",
      "current best loss is 65476: 0.016141\n",
      "67000 Iter: total_loss 0.016564 Chamfer_loss 0.016564\n",
      "current best loss is 65476: 0.016141\n",
      "68000 Iter: total_loss 0.016431 Chamfer_loss 0.016431\n",
      "current best loss is 65476: 0.016141\n",
      "69000 Iter: total_loss 0.016480 Chamfer_loss 0.016480\n",
      "current best loss is 65476: 0.016141\n",
      "70000 Iter: total_loss 0.016371 Chamfer_loss 0.016371\n",
      "current best loss is 69219: 0.016078\n",
      "71000 Iter: total_loss 0.016415 Chamfer_loss 0.016415\n",
      "current best loss is 70828: 0.016064\n",
      "72000 Iter: total_loss 0.016156 Chamfer_loss 0.016156\n",
      "current best loss is 71369: 0.016053\n",
      "73000 Iter: total_loss 0.016393 Chamfer_loss 0.016393\n",
      "current best loss is 71369: 0.016053\n",
      "74000 Iter: total_loss 0.016459 Chamfer_loss 0.016459\n",
      "current best loss is 73262: 0.016042\n",
      "75000 Iter: total_loss 0.016260 Chamfer_loss 0.016260\n",
      "current best loss is 74789: 0.016030\n",
      "76000 Iter: total_loss 0.016447 Chamfer_loss 0.016447\n",
      "current best loss is 75846: 0.016009\n",
      "77000 Iter: total_loss 0.016404 Chamfer_loss 0.016404\n",
      "current best loss is 76463: 0.016009\n",
      "78000 Iter: total_loss 0.016416 Chamfer_loss 0.016416\n",
      "current best loss is 77297: 0.015936\n",
      "79000 Iter: total_loss 0.016322 Chamfer_loss 0.016322\n",
      "current best loss is 77297: 0.015936\n",
      "80000 Iter: total_loss 0.016303 Chamfer_loss 0.016303\n",
      "current best loss is 77297: 0.015936\n",
      "81000 Iter: total_loss 0.016580 Chamfer_loss 0.016580\n",
      "current best loss is 80629: 0.015931\n",
      "82000 Iter: total_loss 0.016162 Chamfer_loss 0.016162\n",
      "current best loss is 81856: 0.015903\n",
      "83000 Iter: total_loss 0.016603 Chamfer_loss 0.016603\n",
      "current best loss is 81856: 0.015903\n",
      "84000 Iter: total_loss 0.016276 Chamfer_loss 0.016276\n",
      "current best loss is 83669: 0.015893\n",
      "85000 Iter: total_loss 0.016283 Chamfer_loss 0.016283\n",
      "current best loss is 84909: 0.015892\n",
      "86000 Iter: total_loss 0.016206 Chamfer_loss 0.016206\n",
      "current best loss is 85591: 0.015828\n",
      "87000 Iter: total_loss 0.016644 Chamfer_loss 0.016644\n",
      "current best loss is 85591: 0.015828\n",
      "88000 Iter: total_loss 0.016234 Chamfer_loss 0.016234\n",
      "current best loss is 87822: 0.015821\n",
      "89000 Iter: total_loss 0.015973 Chamfer_loss 0.015973\n",
      "current best loss is 88039: 0.015805\n",
      "90000 Iter: total_loss 0.015957 Chamfer_loss 0.015957\n",
      "current best loss is 88039: 0.015805\n",
      "91000 Iter: total_loss 0.015999 Chamfer_loss 0.015999\n",
      "current best loss is 88039: 0.015805\n",
      "92000 Iter: total_loss 0.016036 Chamfer_loss 0.016036\n",
      "current best loss is 91459: 0.015757\n",
      "93000 Iter: total_loss 0.015894 Chamfer_loss 0.015894\n",
      "current best loss is 91459: 0.015757\n",
      "94000 Iter: total_loss 0.016320 Chamfer_loss 0.016320\n",
      "current best loss is 91459: 0.015757\n",
      "95000 Iter: total_loss 0.016068 Chamfer_loss 0.016068\n",
      "current best loss is 94416: 0.015745\n",
      "96000 Iter: total_loss 0.016185 Chamfer_loss 0.016185\n",
      "current best loss is 94416: 0.015745\n",
      "97000 Iter: total_loss 0.016162 Chamfer_loss 0.016162\n",
      "current best loss is 94416: 0.015745\n",
      "98000 Iter: total_loss 0.016009 Chamfer_loss 0.016009\n",
      "current best loss is 97943: 0.015684\n",
      "99000 Iter: total_loss 0.015877 Chamfer_loss 0.015877\n",
      "current best loss is 97943: 0.015684\n",
      "100000 Iter: total_loss 0.016034 Chamfer_loss 0.016034\n",
      "current best loss is 99829: 0.015679\n",
      "101000 Iter: total_loss 0.016009 Chamfer_loss 0.016009\n",
      "current best loss is 99829: 0.015679\n",
      "102000 Iter: total_loss 0.016061 Chamfer_loss 0.016061\n",
      "current best loss is 101026: 0.015671\n",
      "103000 Iter: total_loss 0.016169 Chamfer_loss 0.016169\n",
      "current best loss is 101026: 0.015671\n",
      "104000 Iter: total_loss 0.016001 Chamfer_loss 0.016001\n",
      "current best loss is 103789: 0.015657\n",
      "105000 Iter: total_loss 0.015940 Chamfer_loss 0.015940\n",
      "current best loss is 103789: 0.015657\n",
      "106000 Iter: total_loss 0.015831 Chamfer_loss 0.015831\n",
      "current best loss is 103789: 0.015657\n",
      "107000 Iter: total_loss 0.015834 Chamfer_loss 0.015834\n",
      "current best loss is 106053: 0.015646\n",
      "108000 Iter: total_loss 0.016063 Chamfer_loss 0.016063\n",
      "current best loss is 107726: 0.015606\n",
      "109000 Iter: total_loss 0.015819 Chamfer_loss 0.015819\n",
      "current best loss is 108046: 0.015599\n",
      "110000 Iter: total_loss 0.015838 Chamfer_loss 0.015838\n",
      "current best loss is 108046: 0.015599\n",
      "111000 Iter: total_loss 0.015834 Chamfer_loss 0.015834\n",
      "current best loss is 110026: 0.015575\n",
      "112000 Iter: total_loss 0.015752 Chamfer_loss 0.015752\n",
      "current best loss is 110026: 0.015575\n",
      "113000 Iter: total_loss 0.015937 Chamfer_loss 0.015937\n",
      "current best loss is 110026: 0.015575\n",
      "114000 Iter: total_loss 0.015749 Chamfer_loss 0.015749\n",
      "current best loss is 110026: 0.015575\n",
      "115000 Iter: total_loss 0.015793 Chamfer_loss 0.015793\n",
      "current best loss is 110026: 0.015575\n",
      "116000 Iter: total_loss 0.015903 Chamfer_loss 0.015903\n",
      "current best loss is 115009: 0.015559\n",
      "117000 Iter: total_loss 0.016161 Chamfer_loss 0.016161\n",
      "current best loss is 116167: 0.015554\n",
      "118000 Iter: total_loss 0.016270 Chamfer_loss 0.016270\n",
      "current best loss is 117044: 0.015517\n",
      "119000 Iter: total_loss 0.015853 Chamfer_loss 0.015853\n",
      "current best loss is 117044: 0.015517\n",
      "120000 Iter: total_loss 0.015718 Chamfer_loss 0.015718\n",
      "current best loss is 117044: 0.015517\n",
      "121000 Iter: total_loss 0.015782 Chamfer_loss 0.015782\n",
      "current best loss is 120982: 0.015511\n",
      "122000 Iter: total_loss 0.015748 Chamfer_loss 0.015748\n",
      "current best loss is 120982: 0.015511\n",
      "123000 Iter: total_loss 0.015709 Chamfer_loss 0.015709\n",
      "current best loss is 120982: 0.015511\n",
      "124000 Iter: total_loss 0.016228 Chamfer_loss 0.016228\n",
      "current best loss is 123074: 0.015503\n",
      "125000 Iter: total_loss 0.015767 Chamfer_loss 0.015767\n",
      "current best loss is 124437: 0.015472\n",
      "126000 Iter: total_loss 0.015716 Chamfer_loss 0.015716\n",
      "current best loss is 124437: 0.015472\n",
      "127000 Iter: total_loss 0.015694 Chamfer_loss 0.015694\n",
      "current best loss is 124437: 0.015472\n",
      "128000 Iter: total_loss 0.015634 Chamfer_loss 0.015634\n",
      "current best loss is 127061: 0.015454\n",
      "129000 Iter: total_loss 0.015804 Chamfer_loss 0.015804\n",
      "current best loss is 127061: 0.015454\n",
      "130000 Iter: total_loss 0.015815 Chamfer_loss 0.015815\n",
      "current best loss is 127061: 0.015454\n",
      "131000 Iter: total_loss 0.015758 Chamfer_loss 0.015758\n",
      "current best loss is 127061: 0.015454\n",
      "132000 Iter: total_loss 0.015994 Chamfer_loss 0.015994\n",
      "current best loss is 127061: 0.015454\n",
      "133000 Iter: total_loss 0.015864 Chamfer_loss 0.015864\n",
      "current best loss is 132359: 0.015447\n",
      "134000 Iter: total_loss 0.015704 Chamfer_loss 0.015704\n",
      "current best loss is 133616: 0.015427\n",
      "135000 Iter: total_loss 0.015935 Chamfer_loss 0.015935\n",
      "current best loss is 133616: 0.015427\n",
      "136000 Iter: total_loss 0.015753 Chamfer_loss 0.015753\n",
      "current best loss is 133616: 0.015427\n",
      "137000 Iter: total_loss 0.015820 Chamfer_loss 0.015820\n",
      "current best loss is 133616: 0.015427\n",
      "138000 Iter: total_loss 0.015600 Chamfer_loss 0.015600\n",
      "current best loss is 137658: 0.015416\n",
      "139000 Iter: total_loss 0.015747 Chamfer_loss 0.015747\n",
      "current best loss is 138102: 0.015382\n",
      "140000 Iter: total_loss 0.016260 Chamfer_loss 0.016260\n",
      "current best loss is 138102: 0.015382\n",
      "141000 Iter: total_loss 0.015610 Chamfer_loss 0.015610\n",
      "current best loss is 140610: 0.015348\n",
      "142000 Iter: total_loss 0.015823 Chamfer_loss 0.015823\n",
      "current best loss is 140610: 0.015348\n",
      "143000 Iter: total_loss 0.015623 Chamfer_loss 0.015623\n",
      "current best loss is 140610: 0.015348\n",
      "144000 Iter: total_loss 0.015644 Chamfer_loss 0.015644\n",
      "current best loss is 143478: 0.015341\n",
      "145000 Iter: total_loss 0.015813 Chamfer_loss 0.015813\n",
      "current best loss is 144369: 0.015336\n",
      "146000 Iter: total_loss 0.015640 Chamfer_loss 0.015640\n",
      "current best loss is 144369: 0.015336\n",
      "147000 Iter: total_loss 0.015440 Chamfer_loss 0.015440\n",
      "current best loss is 146405: 0.015331\n",
      "148000 Iter: total_loss 0.015567 Chamfer_loss 0.015567\n",
      "current best loss is 147960: 0.015328\n",
      "149000 Iter: total_loss 0.015562 Chamfer_loss 0.015562\n",
      "current best loss is 147960: 0.015328\n",
      "150000 Iter: total_loss 0.016009 Chamfer_loss 0.016009\n",
      "current best loss is 149669: 0.015273\n",
      "151000 Iter: total_loss 0.015742 Chamfer_loss 0.015742\n",
      "current best loss is 149669: 0.015273\n",
      "152000 Iter: total_loss 0.015739 Chamfer_loss 0.015739\n",
      "current best loss is 149669: 0.015273\n",
      "153000 Iter: total_loss 0.015553 Chamfer_loss 0.015553\n",
      "current best loss is 149669: 0.015273\n",
      "154000 Iter: total_loss 0.015723 Chamfer_loss 0.015723\n",
      "current best loss is 149669: 0.015273\n",
      "155000 Iter: total_loss 0.015742 Chamfer_loss 0.015742\n",
      "current best loss is 149669: 0.015273\n",
      "156000 Iter: total_loss 0.015477 Chamfer_loss 0.015477\n",
      "current best loss is 155979: 0.015271\n",
      "157000 Iter: total_loss 0.015560 Chamfer_loss 0.015560\n",
      "current best loss is 155979: 0.015271\n",
      "158000 Iter: total_loss 0.015805 Chamfer_loss 0.015805\n",
      "current best loss is 157561: 0.015256\n",
      "159000 Iter: total_loss 0.015846 Chamfer_loss 0.015846\n",
      "current best loss is 157561: 0.015256\n",
      "160000 Iter: total_loss 0.015699 Chamfer_loss 0.015699\n",
      "current best loss is 159835: 0.015246\n",
      "161000 Iter: total_loss 0.015386 Chamfer_loss 0.015386\n",
      "current best loss is 159835: 0.015246\n",
      "162000 Iter: total_loss 0.015578 Chamfer_loss 0.015578\n",
      "current best loss is 159835: 0.015246\n",
      "163000 Iter: total_loss 0.015479 Chamfer_loss 0.015479\n",
      "current best loss is 162808: 0.015218\n",
      "164000 Iter: total_loss 0.015543 Chamfer_loss 0.015543\n",
      "current best loss is 162808: 0.015218\n",
      "165000 Iter: total_loss 0.016175 Chamfer_loss 0.016175\n",
      "current best loss is 162808: 0.015218\n",
      "166000 Iter: total_loss 0.015426 Chamfer_loss 0.015426\n",
      "current best loss is 165251: 0.015208\n",
      "167000 Iter: total_loss 0.015879 Chamfer_loss 0.015879\n",
      "current best loss is 165251: 0.015208\n",
      "168000 Iter: total_loss 0.015495 Chamfer_loss 0.015495\n",
      "current best loss is 165251: 0.015208\n",
      "169000 Iter: total_loss 0.015475 Chamfer_loss 0.015475\n",
      "current best loss is 168965: 0.015182\n",
      "170000 Iter: total_loss 0.015462 Chamfer_loss 0.015462\n",
      "current best loss is 169613: 0.015180\n",
      "171000 Iter: total_loss 0.015296 Chamfer_loss 0.015296\n",
      "current best loss is 169613: 0.015180\n",
      "172000 Iter: total_loss 0.015412 Chamfer_loss 0.015412\n",
      "current best loss is 169613: 0.015180\n",
      "173000 Iter: total_loss 0.015366 Chamfer_loss 0.015366\n",
      "current best loss is 172120: 0.015179\n",
      "174000 Iter: total_loss 0.015388 Chamfer_loss 0.015388\n",
      "current best loss is 172120: 0.015179\n",
      "175000 Iter: total_loss 0.015381 Chamfer_loss 0.015381\n",
      "current best loss is 172120: 0.015179\n",
      "176000 Iter: total_loss 0.015518 Chamfer_loss 0.015518\n",
      "current best loss is 175531: 0.015160\n",
      "177000 Iter: total_loss 0.015465 Chamfer_loss 0.015465\n",
      "current best loss is 175531: 0.015160\n",
      "178000 Iter: total_loss 0.015517 Chamfer_loss 0.015517\n",
      "current best loss is 177704: 0.015158\n",
      "179000 Iter: total_loss 0.015379 Chamfer_loss 0.015379\n",
      "current best loss is 178567: 0.015154\n",
      "180000 Iter: total_loss 0.015362 Chamfer_loss 0.015362\n",
      "current best loss is 179433: 0.015128\n",
      "181000 Iter: total_loss 0.015405 Chamfer_loss 0.015405\n",
      "current best loss is 180856: 0.015118\n",
      "182000 Iter: total_loss 0.015485 Chamfer_loss 0.015485\n",
      "current best loss is 180856: 0.015118\n",
      "183000 Iter: total_loss 0.015467 Chamfer_loss 0.015467\n",
      "current best loss is 180856: 0.015118\n",
      "184000 Iter: total_loss 0.015436 Chamfer_loss 0.015436\n",
      "current best loss is 180856: 0.015118\n",
      "185000 Iter: total_loss 0.015323 Chamfer_loss 0.015323\n",
      "current best loss is 180856: 0.015118\n",
      "186000 Iter: total_loss 0.015290 Chamfer_loss 0.015290\n",
      "current best loss is 180856: 0.015118\n",
      "187000 Iter: total_loss 0.015440 Chamfer_loss 0.015440\n",
      "current best loss is 180856: 0.015118\n",
      "188000 Iter: total_loss 0.015448 Chamfer_loss 0.015448\n",
      "current best loss is 180856: 0.015118\n",
      "189000 Iter: total_loss 0.015488 Chamfer_loss 0.015488\n",
      "current best loss is 180856: 0.015118\n",
      "190000 Iter: total_loss 0.015297 Chamfer_loss 0.015297\n",
      "current best loss is 180856: 0.015118\n",
      "191000 Iter: total_loss 0.015397 Chamfer_loss 0.015397\n",
      "current best loss is 190959: 0.015072\n",
      "192000 Iter: total_loss 0.015513 Chamfer_loss 0.015513\n",
      "current best loss is 190959: 0.015072\n",
      "193000 Iter: total_loss 0.015558 Chamfer_loss 0.015558\n",
      "current best loss is 192237: 0.015061\n",
      "194000 Iter: total_loss 0.015610 Chamfer_loss 0.015610\n",
      "current best loss is 192237: 0.015061\n",
      "195000 Iter: total_loss 0.015235 Chamfer_loss 0.015235\n",
      "current best loss is 192237: 0.015061\n",
      "196000 Iter: total_loss 0.015274 Chamfer_loss 0.015274\n",
      "current best loss is 192237: 0.015061\n",
      "197000 Iter: total_loss 0.015282 Chamfer_loss 0.015282\n",
      "current best loss is 192237: 0.015061\n",
      "198000 Iter: total_loss 0.015365 Chamfer_loss 0.015365\n",
      "current best loss is 192237: 0.015061\n",
      "199000 Iter: total_loss 0.015361 Chamfer_loss 0.015361\n",
      "current best loss is 198447: 0.015046\n",
      "200000 Iter: total_loss 0.015223 Chamfer_loss 0.015223\n",
      "current best loss is 198447: 0.015046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best = None\n",
    "best_loss = 0\n",
    "best_iter = 0\n",
    "\n",
    "for i in range(Niter):\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "    sv, sf = src_mesh.get_mesh_verts_faces(0)\n",
    "    sn = src_mesh.verts_normals_packed()\n",
    "    inputs = torch.cat([sv.cuda(),sn.cuda()],1)\n",
    "    deform_verts = models(inputs) \n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "    f1 = new_src_mesh.faces_packed()\n",
    "    v1 = new_src_mesh.verts_packed()\n",
    "    c1,l1,n1,_ = CompCLNn(f1,v1)\n",
    "    c2,l2,n2,_ = CompCLNn(faces_idx2,verts2)\n",
    "\n",
    "    c1 = torch.cat([c1,n1],1)\n",
    "    c2 = torch.cat([c2,n2],1)\n",
    "    \n",
    "    loss_chamfer, _ = chamfer_distance(c1.unsqueeze(0), c2.unsqueeze(0))\n",
    "    \n",
    "    # Weighted sum of the losses\n",
    "    loss = w_chamfer*loss_chamfer\n",
    "\n",
    "    if best_loss == 0:\n",
    "        best = deform_verts\n",
    "        best_loss = loss.detach()\n",
    "        best_iter = 0\n",
    "    elif best_loss > loss.detach():\n",
    "        best = deform_verts\n",
    "        best_loss = loss.detach()\n",
    "        best_iter = i\n",
    "        torch.save(models.state_dict(),'../../results/chamfer_%s_n.pth'%experiment_name)\n",
    "\n",
    "    # Print the losses\n",
    "    if i % plot_period==0:\n",
    "        print('%d Iter: total_loss %.6f Chamfer_loss %.6f'% (i,loss,loss_chamfer))\n",
    "        print('current best loss is %d: %.6f'%(best_iter,best_loss))\n",
    "        \n",
    "    # Optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Fetch the verts and faces of the final predicted mesh\n",
    "new_src_mesh = src_mesh.offset_verts(best)\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "\n",
    "# Scale normalize back to the original target size\n",
    "final_verts = (final_verts) * scale2 + center2\n",
    "\n",
    "# Store the predicted mesh using save_obj\n",
    "save_obj('../../results/chamfer_%s_low.obj'%(experiment_name), final_verts, final_faces)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final Chamfer distance: 0.001417\n"
     ]
    }
   ],
   "source": [
    "final_chamfer,_ = chamfer_distance((final_verts.unsqueeze(0).double() - center2)/scale2, verts2.unsqueeze(0).double())\n",
    "print('final Chamfer distance: %.6f'%(final_chamfer.detach().cpu().numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
