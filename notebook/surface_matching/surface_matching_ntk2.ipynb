{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '../..' )\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.io import load_obj, save_obj, load_ply\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance, \n",
    "    mesh_edge_loss, \n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "import numpy as np\n",
    "from energy import tangent_kernel\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be slow!\")\n",
    "import input_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_name = 'hippo'\n",
    "experiment_name = 'cup'\n",
    "#experiment_name = 'dolphin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No functional values found: set to 0\n",
      "No functional values found: set to 0\n",
      "(15146, 3)\n",
      "(15004, 3)\n",
      "(2839, 3)\n",
      "(2845, 3)\n"
     ]
    }
   ],
   "source": [
    "# We read the target 3D model using load_obj\n",
    "# case 1.  \"Cup\"\n",
    "if experiment_name=='cup':\n",
    "    # two different Cups \n",
    "    [VS,FS,FunS] = input_output.loadData(\"../../data/matching/cup2.ply\")\n",
    "    [VT,FT,FunT] = input_output.loadData(\"../../data/matching/cup3_broken.ply\")\n",
    "    source = [VS,FS]\n",
    "    target= [VT,FT]\n",
    "    # original size\n",
    "    print(VS.shape)\n",
    "    print(VT.shape)\n",
    "    # Option 1 Sampling\n",
    "    # Decimate source mesh to compute initialization for the multires algorithm \n",
    "    param_decimation = {'factor':13/16,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 16\n",
    "    [verts1,faces1]= input_output.decimate_mesh(VS,FS,param_decimation)\n",
    "    print(verts1.shape)\n",
    "    param_decimation = {'factor':13/16,'Vol_preser':1, 'Fun_Error_Metric': 1, 'Fun_weigth':0.00} #decimate by a factor of 16\n",
    "    [verts2,faces2]= input_output.decimate_mesh(VT,FT,param_decimation)\n",
    "    print(verts2.shape)\n",
    "    verts1 = torch.FloatTensor(verts1)\n",
    "    verts2 = torch.FloatTensor(verts2)\n",
    "    faces1 = torch.LongTensor(faces1)\n",
    "    faces2 = torch.LongTensor(faces2)\n",
    "# Case 2 Dolphin\n",
    "elif experiment_name == 'dolphin':\n",
    "    # sphere to dolphine \n",
    "    src_mesh = ico_sphere(4)\n",
    "    VT, FT, FunS = load_obj(\"../../data/matching/dolphin.obj\")\n",
    "    VS, FS = src_mesh.verts_packed(), src_mesh.faces_packed()\n",
    "    verts1, faces1 = VS, FS\n",
    "    verts2, faces2 = VT, FT.verts_idx\n",
    "# Case 3 Hippocampus \n",
    "elif experiment_name == 'hippo':\n",
    "    verts1, faces1, verts2, faces2 = torch.load('../../data/matching/hippos_red.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testnet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(3,64),nn.ReLU(),nn.Linear(64,128),nn.ReLU(),nn.Linear(128,3))\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2839, 3])\n",
      "torch.Size([2845, 3])\n"
     ]
    }
   ],
   "source": [
    "# verts is a FloatTensor of shape (V, 3) where V is the number of vertices in the mesh\n",
    "# faces is an object which contains the following LongTensors: verts_idx, normals_idx and textures_idx\n",
    "# For this tutorial, normals and textures are ignored.\n",
    "faces_idx1 = faces1.to(device)\n",
    "faces_idx2 = faces2.to(device)\n",
    "\n",
    "# Mark: obj files\n",
    "#faces_idx1 = faces1.to(device)#.verts_idx.to(device)\n",
    "#faces_idx2 = faces2.verts_idx.to(device)\n",
    "\n",
    "verts1 = verts1.to(device)\n",
    "verts2 = verts2.to(device)\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 centered at (0,0,0). \n",
    "# (scale, center) will be used to bring the predicted mesh to its original center and scale\n",
    "# Note that normalizing the target mesh, speeds up the optimization but is not necessary!\n",
    "#'''\n",
    "center1 = verts1.mean(0)\n",
    "center2 = verts2.mean(0)\n",
    "verts1 = verts1 - center1\n",
    "verts2 = verts2 - center2\n",
    "scale1 = max(verts1.abs().max(0)[0])\n",
    "scale2 = max(verts2.abs().max(0)[0])\n",
    "verts1 = verts1 / scale1\n",
    "verts2 = verts2 / scale2\n",
    "#'''\n",
    "# We construct a Meshes structure for the target mesh\n",
    "src_mesh = Meshes(verts=[verts1], faces=[faces_idx1])\n",
    "trg_mesh = Meshes(verts=[verts2], faces=[faces_idx2])\n",
    "print(verts1.shape)\n",
    "print(verts2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setting\n",
    "#deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "#optimizer = torch.optim.SGD([deform_verts], lr=.01, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam([deform_verts], lr=.1)\n",
    "models = testnet().cuda()\n",
    "optimizer = torch.optim.Adam(models.parameters(), lr=.001)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[501,1001], gamma=0.1)\n",
    "\n",
    "# Number of optimization steps\n",
    "Niter = 20001\n",
    "\n",
    "# loss parameters\n",
    "\n",
    "# weight for varifold loss\n",
    "w_varifold = 200.0 \n",
    "# Weight for the chamfer loss\n",
    "w_chamfer = 1.0 \n",
    "# Weight for mesh edge loss\n",
    "w_edge = 1.0 \n",
    "# Weight for mesh normal consistency\n",
    "w_normal = 0.1 \n",
    "# Weight for mesh laplacian smoothing\n",
    "w_laplacian = 0.1 \n",
    "# Plot period for the losses\n",
    "plot_period = 250\n",
    "\n",
    "\n",
    "chamfer_losses = []\n",
    "laplacian_losses = []\n",
    "edge_losses = []\n",
    "normal_losses = []\n",
    "\n",
    "varifold = tangent_kernel(1,1.,0.05,3,mode='NTK2')\n",
    "def compute_engine(V1,V2,L1,L2,K):\n",
    "    cst_tmp = []\n",
    "    n_batch = 10000#4096\n",
    "    for i in range(len(V1)//n_batch + 1):\n",
    "        tmp = V1[i*n_batch:(i+1)*n_batch,:]\n",
    "        l_tmp = L1[i*n_batch:(i+1)*n_batch,:]\n",
    "        v = torch.matmul(K(tmp,V2)*100000,L2)*l_tmp\n",
    "        cst_tmp.append(v)\n",
    "    cst = torch.sum(torch.cat(cst_tmp,0))\n",
    "    return cst\n",
    "\n",
    "def CompCLNn(F, V):\n",
    "    if F.shape[1] == 2:\n",
    "        V0, V1 = V.index_select(0, F[:, 0]), V.index_select(0, F[:, 1])\n",
    "        C, N  =  (V0 + V1)/2, V1 - V0\n",
    "    else:\n",
    "        V0, V1, V2 = V.index_select(0, F[:, 0]), V.index_select(0, F[:, 1]), V.index_select(0, F[:, 2])\n",
    "        C, N =  (V0 + V1 + V2)/3, .5 * torch.cross(V1 - V0, V2 - V0)\n",
    "\n",
    "    L = (N ** 2).sum(dim=1)[:, None].sqrt()\n",
    "    return C, L, N / L, 1#Fun_F\n",
    "\n",
    "c,l,n,_ = CompCLNn(faces_idx1,verts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Iter: total_loss 13829375.000000 Chamfer_loss 0.294697 Varifold loss 69146.87500000\n",
      "250 Iter: total_loss 156750.000000 Chamfer_loss 0.032332 Varifold loss 783.75000000\n",
      "500 Iter: total_loss 82900.000000 Chamfer_loss 0.026433 Varifold loss 414.50000000\n",
      "750 Iter: total_loss 66400.000000 Chamfer_loss 0.024968 Varifold loss 332.00000000\n",
      "1000 Iter: total_loss 62500.000000 Chamfer_loss 0.023989 Varifold loss 312.50000000\n",
      "1250 Iter: total_loss 53000.000000 Chamfer_loss 0.023751 Varifold loss 265.00000000\n",
      "1500 Iter: total_loss 50650.000000 Chamfer_loss 0.023522 Varifold loss 253.25000000\n",
      "1750 Iter: total_loss 51450.000000 Chamfer_loss 0.022737 Varifold loss 257.25000000\n",
      "2000 Iter: total_loss 43700.000000 Chamfer_loss 0.022592 Varifold loss 218.50000000\n",
      "2250 Iter: total_loss 42300.000000 Chamfer_loss 0.022534 Varifold loss 211.50000000\n",
      "2500 Iter: total_loss 40500.000000 Chamfer_loss 0.022209 Varifold loss 202.50000000\n",
      "2750 Iter: total_loss 39500.000000 Chamfer_loss 0.021825 Varifold loss 197.50000000\n",
      "3000 Iter: total_loss 38500.000000 Chamfer_loss 0.021477 Varifold loss 192.50000000\n",
      "3250 Iter: total_loss 37300.000000 Chamfer_loss 0.021067 Varifold loss 186.50000000\n",
      "3500 Iter: total_loss 36800.000000 Chamfer_loss 0.020754 Varifold loss 184.00000000\n",
      "3750 Iter: total_loss 35700.000000 Chamfer_loss 0.020423 Varifold loss 178.50000000\n",
      "4000 Iter: total_loss 35100.000000 Chamfer_loss 0.020056 Varifold loss 175.50000000\n",
      "4250 Iter: total_loss 34400.000000 Chamfer_loss 0.019937 Varifold loss 172.00000000\n",
      "4500 Iter: total_loss 33800.000000 Chamfer_loss 0.019818 Varifold loss 169.00000000\n",
      "4750 Iter: total_loss 46250.000000 Chamfer_loss 0.019787 Varifold loss 231.25000000\n",
      "5000 Iter: total_loss 32950.000000 Chamfer_loss 0.019668 Varifold loss 164.75000000\n",
      "5250 Iter: total_loss 42800.000000 Chamfer_loss 0.019569 Varifold loss 214.00000000\n",
      "5500 Iter: total_loss 34800.000000 Chamfer_loss 0.019459 Varifold loss 174.00000000\n",
      "5750 Iter: total_loss 32300.000000 Chamfer_loss 0.019257 Varifold loss 161.50000000\n",
      "6000 Iter: total_loss 31800.000000 Chamfer_loss 0.019027 Varifold loss 159.00000000\n",
      "6250 Iter: total_loss 31100.000000 Chamfer_loss 0.019033 Varifold loss 155.50000000\n",
      "6500 Iter: total_loss 31250.000000 Chamfer_loss 0.018924 Varifold loss 156.25000000\n",
      "6750 Iter: total_loss 30800.000000 Chamfer_loss 0.018672 Varifold loss 154.00000000\n",
      "7000 Iter: total_loss 30500.000000 Chamfer_loss 0.018697 Varifold loss 152.50000000\n",
      "7250 Iter: total_loss 33100.000000 Chamfer_loss 0.018697 Varifold loss 165.50000000\n",
      "7500 Iter: total_loss 31400.000000 Chamfer_loss 0.018743 Varifold loss 157.00000000\n",
      "7750 Iter: total_loss 32400.000000 Chamfer_loss 0.018662 Varifold loss 162.00000000\n",
      "8000 Iter: total_loss 34450.000000 Chamfer_loss 0.018749 Varifold loss 172.25000000\n",
      "8250 Iter: total_loss 29900.000000 Chamfer_loss 0.018664 Varifold loss 149.50000000\n",
      "8500 Iter: total_loss 30100.000000 Chamfer_loss 0.018703 Varifold loss 150.50000000\n",
      "8750 Iter: total_loss 29700.000000 Chamfer_loss 0.018740 Varifold loss 148.50000000\n",
      "9000 Iter: total_loss 30450.000000 Chamfer_loss 0.018711 Varifold loss 152.25000000\n",
      "9250 Iter: total_loss 30950.000000 Chamfer_loss 0.018697 Varifold loss 154.75000000\n",
      "9500 Iter: total_loss 29350.000000 Chamfer_loss 0.018652 Varifold loss 146.75000000\n",
      "9750 Iter: total_loss 30400.000000 Chamfer_loss 0.018669 Varifold loss 152.00000000\n",
      "10000 Iter: total_loss 30750.000000 Chamfer_loss 0.018662 Varifold loss 153.75000000\n",
      "10250 Iter: total_loss 35600.000000 Chamfer_loss 0.018645 Varifold loss 178.00000000\n",
      "10500 Iter: total_loss 29000.000000 Chamfer_loss 0.018654 Varifold loss 145.00000000\n",
      "10750 Iter: total_loss 29100.000000 Chamfer_loss 0.018603 Varifold loss 145.50000000\n",
      "11000 Iter: total_loss 28900.000000 Chamfer_loss 0.018581 Varifold loss 144.50000000\n",
      "11250 Iter: total_loss 28650.000000 Chamfer_loss 0.018633 Varifold loss 143.25000000\n",
      "11500 Iter: total_loss 33150.000000 Chamfer_loss 0.018659 Varifold loss 165.75000000\n",
      "11750 Iter: total_loss 28650.000000 Chamfer_loss 0.018652 Varifold loss 143.25000000\n",
      "12000 Iter: total_loss 31650.000000 Chamfer_loss 0.018648 Varifold loss 158.25000000\n",
      "12250 Iter: total_loss 28100.000000 Chamfer_loss 0.018635 Varifold loss 140.50000000\n",
      "12500 Iter: total_loss 29450.000000 Chamfer_loss 0.018676 Varifold loss 147.25000000\n",
      "12750 Iter: total_loss 29000.000000 Chamfer_loss 0.018699 Varifold loss 145.00000000\n",
      "13000 Iter: total_loss 27900.000000 Chamfer_loss 0.018769 Varifold loss 139.50000000\n",
      "13250 Iter: total_loss 27900.000000 Chamfer_loss 0.018795 Varifold loss 139.50000000\n",
      "13500 Iter: total_loss 29450.000000 Chamfer_loss 0.018796 Varifold loss 147.25000000\n",
      "13750 Iter: total_loss 29500.000000 Chamfer_loss 0.018777 Varifold loss 147.50000000\n",
      "14000 Iter: total_loss 27700.000000 Chamfer_loss 0.018771 Varifold loss 138.50000000\n",
      "14250 Iter: total_loss 28150.000000 Chamfer_loss 0.018689 Varifold loss 140.75000000\n",
      "14500 Iter: total_loss 27950.000000 Chamfer_loss 0.018740 Varifold loss 139.75000000\n",
      "14750 Iter: total_loss 27800.000000 Chamfer_loss 0.018668 Varifold loss 139.00000000\n",
      "15000 Iter: total_loss 27800.000000 Chamfer_loss 0.018645 Varifold loss 139.00000000\n",
      "15250 Iter: total_loss 27500.000000 Chamfer_loss 0.018659 Varifold loss 137.50000000\n",
      "15500 Iter: total_loss 28850.000000 Chamfer_loss 0.018689 Varifold loss 144.25000000\n",
      "15750 Iter: total_loss 27800.000000 Chamfer_loss 0.018707 Varifold loss 139.00000000\n",
      "16000 Iter: total_loss 27500.000000 Chamfer_loss 0.018683 Varifold loss 137.50000000\n",
      "16250 Iter: total_loss 32150.000000 Chamfer_loss 0.018696 Varifold loss 160.75000000\n",
      "16500 Iter: total_loss 27400.000000 Chamfer_loss 0.018667 Varifold loss 137.00000000\n",
      "16750 Iter: total_loss 27250.000000 Chamfer_loss 0.018655 Varifold loss 136.25000000\n",
      "17000 Iter: total_loss 27300.000000 Chamfer_loss 0.018647 Varifold loss 136.50000000\n",
      "17250 Iter: total_loss 27500.000000 Chamfer_loss 0.018658 Varifold loss 137.50000000\n",
      "17500 Iter: total_loss 42100.000000 Chamfer_loss 0.018681 Varifold loss 210.50000000\n",
      "17750 Iter: total_loss 28800.000000 Chamfer_loss 0.018680 Varifold loss 144.00000000\n",
      "18000 Iter: total_loss 27600.000000 Chamfer_loss 0.018742 Varifold loss 138.00000000\n",
      "18250 Iter: total_loss 27650.000000 Chamfer_loss 0.018709 Varifold loss 138.25000000\n",
      "18500 Iter: total_loss 27400.000000 Chamfer_loss 0.018696 Varifold loss 137.00000000\n",
      "18750 Iter: total_loss 29700.000000 Chamfer_loss 0.018692 Varifold loss 148.50000000\n",
      "19000 Iter: total_loss 27300.000000 Chamfer_loss 0.018655 Varifold loss 136.50000000\n",
      "19250 Iter: total_loss 27200.000000 Chamfer_loss 0.018629 Varifold loss 136.00000000\n",
      "19500 Iter: total_loss 27850.000000 Chamfer_loss 0.018619 Varifold loss 139.25000000\n",
      "19750 Iter: total_loss 27350.000000 Chamfer_loss 0.018604 Varifold loss 136.75000000\n",
      "20000 Iter: total_loss 27850.000000 Chamfer_loss 0.018568 Varifold loss 139.25000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(Niter):\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Deform the mesh\n",
    "    sv, sf = src_mesh.get_mesh_verts_faces(0)\n",
    "    deform_verts = models(sv.cuda()) \n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "    f1 = new_src_mesh.faces_packed()\n",
    "    v1 = new_src_mesh.verts_packed()\n",
    "    # We sample 5k points from the surface of each mesh \n",
    "    c1,l1,n1,_ = CompCLNn(f1,v1)\n",
    "    c2,l2,n2,_ = CompCLNn(faces_idx2,verts2)\n",
    "    c1 = torch.cat([c1,n1],1)\n",
    "    c2 = torch.cat([c2,n2],1)\n",
    "    v11 = compute_engine(c1,c1,l1,l1,varifold)\n",
    "    v22 = compute_engine(c2,c2,l2,l2,varifold) \n",
    "    v12 = compute_engine(c2,c1,l2,l1,varifold)\n",
    "    loss_varifold = v11 + v22 -2*v12\n",
    "    \n",
    "    # loss chamfer as reference\n",
    "    loss_chamfer, _ = chamfer_distance(c1.unsqueeze(0), c2.unsqueeze(0))\n",
    "    \n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "    \n",
    "    # mesh normal consistency\n",
    "    loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "    #print(loss_normal)\n",
    "    # mesh laplacian smoothing\n",
    "    loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "    # Weighted sum of the losses\n",
    "    # note PointNet-NTK1 param = 20 & learning rate 0.05 PointNet-NTK2 = 10 learning rate 0.1\n",
    "    loss = w_varifold *loss_varifold \n",
    "    # Print the losses\n",
    "    if i % plot_period==0:\n",
    "        print('%d Iter: total_loss %.6f Chamfer_loss %.6f Varifold loss %.8f'% (i,loss,loss_chamfer, loss_varifold))\n",
    "    \n",
    "    # Save the losses for plotting\n",
    "    chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "    edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "    normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "    laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "        \n",
    "    # Optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Fetch the verts and faces of the final predicted mesh\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "\n",
    "# Scale normalize back to the original target size\n",
    "final_verts = (final_verts) * scale2 + center2\n",
    "\n",
    "# Store the predicted mesh using save_obj\n",
    "#save_obj('./chamfer.obj', final_verts, final_faces)\n",
    "#save_obj('./charon_trouve.obj', final_verts, final_faces)\n",
    "save_obj('../../results/pointnet_ntk2_%s.obj'%experiment_name, final_verts, final_faces)\n",
    "#save_obj('./pointnet_ntk2.obj', final_verts, final_faces)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final Chamfer distance: 0.003567\n"
     ]
    }
   ],
   "source": [
    "final_chamfer,_ = chamfer_distance((final_verts.unsqueeze(0).double() - center2)/scale2, verts2.unsqueeze(0).double())\n",
    "print('final Chamfer distance: %.6f'%(final_chamfer.detach().cpu().numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
